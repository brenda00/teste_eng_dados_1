# üõ†Ô∏è Projeto de Engenharia de Dados 

Este projeto implementa um pipeline completo de Engenharia de Dados utilizando PySpark e servi√ßos AWS. A arquitetura segue o modelo de **camadas (bronze, silver e gold)**, utilizando uma **Landing Zone**.

---

## Vis√£o Geral

- **Origem dos dados:** CSV iniciamente local em /datasets/clientes_sinteticos.csv que popula o S3 etlproj-landing-zone/arquivo/ via terraform
- **Destino:** Data Lake em Amazon S3
- **Transforma√ß√£o:** PySpark com AWS Glue
- **Armazenamento:** Arquitetura em camadas (lading-zone ‚Üí bronze ‚Üí silver ‚Üí gold)
- **Cat√°logo de dados:** Crawlers + AWS Glue Data Catalog
- **Consultas:** AWS Athena

---

## Etapas do Projeto

### 1. `1.ETL/script.py`
- **Objetivo:** realizar o pipeline de extra√ß√£o, transforma√ß√£o e carga.
- Leitura do arquivo `clientes_sinteticos.csv` da Landing Zone (S3).
- Escrita na camada **bronze** com tratamento para deixar o nome dos clientes com letra maiuscula e renomear a coluna de telefone do cliente.
- Deduplica√ß√£o para a camada **silver** tratativa na coluna de telefone para padr√£o (NN)NNNNN-NNNN.
- Escrita em Parquet particionado por `anomesdia` com acesso via Glue Catalog

---

### 2. `2.AnaliseDados/analise.py`
- **Objetivo:** executar an√°lises sobre o conjunto tratado.
- Top 5 clientes com mais atualiza√ß√µes
- M√©dia de idade dos clientes ativos

---

### 3. `3.DesenhodeArquitetura/`
- **Objetivo:** apresentar visualmente a arquitetura da solu√ß√£o a seguir:

Proponha uma arquitetura na AWS para coletar dados de cadastros de clientes em um banco MySQL. Esses dados devem ser persistidos em um datalake que usa a arquitetura medalh√£o:

Desenhe um sistema para coletar dados do banco MySQL realizando CDC.
O processamento e escrita deve ser projetado para os 3 niveis do lake (bronze, silver e gold)
Al√©m do armazenamento do dado ser√° necessaria uma governan√ßa de acesso a n√≠vel de usu√°rio
 
![Desenho da Arquitetura](desenhoArquitetura/desenho_arquitetura.png)


---

### 4. `4.DataQuality/script_data_quality.py`
- **Objetivo:** validar as dimens√µes de qualidade dos dados na camada Silver
- Valida√ß√µes aplicadas:
  - Completude: Verifica se campos obrigat√≥rios est√£o nulos
  - Unicidade: Verifica cod_cliente duplicado
  - Acur√°cia: Valida√ß√£o de formato de telefone
  - Consist√™ncia: Valida√ß√£o de valores negativos em vl_renda
  - Validade: Nomes inv√°lidos + datas futuras

---

### 5. `5.TesteUnitario/`
- **Objetivo:** validar fun√ß√µes cr√≠ticas da transforma√ß√£o
- Testes com `unittest` para fun√ß√£o, valida√ß√£o de telefone
- Casos cobertos:
  - Happy Path
  - Casos extremos (valores nulos, espa√ßos, repeti√ß√µes)
  - Casos inv√°lidos

---

### 6. `6.InfraAsCode/`
-  **Objetivo:** provisionar infraestrutura na AWS via Terraform
- Cria√ß√£o de buckets (landing-zone, bronze, silver, scripts)
- Cria√ß√£o de Glue Job, Glue Crawlers
- Pol√≠ticas de IAM
- Arquivos:
  - `glue.tf`: job, crawler, banco de dados Glue
  - `storage.tf`: buckets, ACLs e criptografia
  - `iam.tf`: roles, policies e acessos

---

## üíª Execu√ß√£o 

### Etapas

```bash
# 1. Ative o ambiente
python -m venv venv
venv\Scripts\activate

# 2. Instale os pacotes
pip install -r requirements.txt

# 3. Execute o terraform
cd terraform
terraform init
terraform plan
terraform apply

# 4. Execu√ß√£o
 1. ETL/script.py (Terraform - AWS Glue Job)
 2. AnaliseDados/analise.py (Terraform - AWS Glue Job Notebook)
 3. DataQuality/script_data_quality.py (Terraform - AWS Glue Job)
 4. python 5.TesteUnitario/unit_test.py (local)
